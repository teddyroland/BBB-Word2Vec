{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "This lesson is designed to explore features of word embeddings produced through the word2vec model.\n",
    "\n",
    "The primary corpus we use consists of the <a href=\"http://txtlab.org/?p=601\">150 English-language novels</a> made available by the <em>.txtLab</em> at McGill University. We also look at a <a href=\"http://ryanheuser.org/word-vectors-1/\">Word2Vec model trained on the ECCO-TCP corpus</a> of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that I have shortened the number of terms in the model by half in order to conserve memory.)\n",
    "\n",
    "For further background on Word2Vec's mechanics, I suggest this <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html\">brief tutorial</a> by Google, especially the sections \"Motivation,\" \"Skip-Gram Model,\" and \"Visualizing.\"\n",
    "\n",
    "### Workshop Agenda\n",
    "<ol>\n",
    "<li>Import & Pre-Processing</li>\n",
    "<li>Word2Vec</li>\n",
    "<ol><li>Training</li>\n",
    "<li>Embeddings</li>\n",
    "<li>Visualization</li>\n",
    "</ol>\n",
    "<li>Saving/Loading Models</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.manifold import MDS, TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom Tokenizer for Classroom Use\n",
    "\n",
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    from string import punctuation\n",
    "    \n",
    "    lower_case = text.lower()\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import & Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Description\n",
    "English-language subset of Andrew Piper's novel corpus, totaling 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n",
    "\n",
    "### Metadata Columns\n",
    "<ol><li>Filename: Name of file on disk</li>\n",
    "<li>ID: Unique ID in Piper corpus</li>\n",
    "<li>Language: Language of novel</li>\n",
    "<li>Date: Initial publication date</li>\n",
    "<li>Title: Title of novel</li>\n",
    "<li>Gender: Authorial gender</li>\n",
    "<li>Person: Textual perspective</li>\n",
    "<li>Length: Number of tokens in novel</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Metadata into Pandas Dataframe\n",
    "\n",
    "meta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check Metadata\n",
    "\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set location of corpus folder\n",
    "\n",
    "fiction_folder = 'txtlab_Novel450_English/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect the text of each file in the 'fiction_folder' on the hard drive\n",
    "\n",
    "# Create empty list, each entry will be the string for a given novel\n",
    "novel_list = []\n",
    "\n",
    "# Iterate through filenames in 'fiction_folder'\n",
    "for filename in os.listdir(fiction_folder):\n",
    "    \n",
    "    # Read novel text as single string\n",
    "    with open(fiction_folder + filename, 'r') as file_in:\n",
    "        this_novel = file_in.read()\n",
    "    \n",
    "    # Add novel text as single string to master list\n",
    "    novel_list.append(this_novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect first item in novel_list\n",
    "\n",
    "novel_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "Word2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we  want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\n",
    "\n",
    "Since novels were imported as single strings, we'll first use <i>sent_tokenize</i> to divide them into sentences, and second, we'll split each sentence into its own list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split each novel into sentences\n",
    "\n",
    "sentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect first sentence\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split each sentence into tokens\n",
    "\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove any sentences that contain zero tokens\n",
    "\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect first sentence\n",
    "\n",
    "words_by_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Embedding\n",
    "Word2Vec is the most prominent word embedding algorithm. Word embedding generally attempts to identify semantic relationships between words by observing them in context.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of Word2Vec are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. Skip-Gram takes a word of interest as its input (e.g. \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\"). CBOW does the opposite, taking the context words (\"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Size: Number of dimensions for word embedding model</li>\n",
    "<li>Window: Number of context words to observe in each direction</li>\n",
    "<li>min_count: Minimum frequency for words included in model</li>\n",
    "<li>sg (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram</li>\n",
    "<li>Alpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Iterations: Number of passes through dataset</li>\n",
    "<li>Batch Size: Number of words to sample from data during each pass</li>\n",
    "</ul>\n",
    "\n",
    "Note: Script uses default value for each argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train word2vec model from txtLab corpus\n",
    "\n",
    "model = gensim.models.Word2Vec(words_by_sentence, size=100, window=5, \\\n",
    "                               min_count=25, sg=1, alpha=0.025, iter=5, batch_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return dense word vector\n",
    "\n",
    "model['whale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-Space Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "Since words are represented as dense vectors, we can ask how similiar words' meanings are based on their cosine similarity (essentially how much they overlap). <em>gensim</em> has a few dout-of-the-box functions that enable different kinds of comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cosine distance between two given word vectors\n",
    "\n",
    "model.similarity('pride','prejudice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find nearest word vectors by cosine distance\n",
    "\n",
    "model.most_similar('pride')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cosine distance between two clusters of word vectors\n",
    "# Each cluster is measured as the mean of its words\n",
    "\n",
    "model.n_similarity(['pride','prejudice'],['whale','harpoon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a list of words, we can ask which doesn't belong\n",
    "\n",
    "# Finds mean vector of words in list\n",
    "# and identifies the word further from that mean\n",
    "\n",
    "model.doesnt_match(['pride','prejudice', 'whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Valences\n",
    "A word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of <em>river bank</em> from the word <em>bank</em>. This would be written mathetmatically as <em>RIVER - BANK</em>, which in <em>gensim</em>'s interface lists <em>RIVER</em> as a positive meaning and <em>BANK</em> as a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get most similar words to BANK, in order\n",
    "# to get a sense for its primary meaning\n",
    "\n",
    "model.most_similar('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the sense of \"river bank\" from \"bank\" and see what is left\n",
    "\n",
    "model.most_similar(positive=['bank'], negative=['river'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy\n",
    "Analogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy <em>MAN is to KING as WOMAN is to ??</em> is rendered as <em>KING - MAN + WOMAN</em>. In the gensim interface, we designate <em>KING</em> and <em>WOMAN</em> as positive terms and <em>MAN</em> as a negative term, since it is subtracted from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get most similar words to KING, in order\n",
    "# to get a sense for its primary meaning\n",
    "\n",
    "model.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The canonic word2vec analogy: King - Man + Woman -> Queen\n",
    "\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gendered Vectors\n",
    "Can we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feminine Vector\n",
    "\n",
    "model.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Masculine Vector\n",
    "\n",
    "model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Use the most_similar method to find the tokens nearest to 'car' in our model.\n",
    "##     Do the same for 'motorcar'.\n",
    "\n",
    "## Q.  What characterizes each word in our corpus? Does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n",
    "\n",
    "## Q.  What has our model learned about nation-states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['spain', 'paris'], negative=['madrid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Perform the canonic Word2Vec addition again but leave out a term:\n",
    "##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n",
    "\n",
    "## Q.  What do these indicate semantically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of words in model\n",
    "\n",
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the whole vocabulary would make it hard to read\n",
    "\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For interpretability, we'll select words that already have a semantic relation\n",
    "\n",
    "her_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n",
    "                                                       negative=['he','him','his','himself'], topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect list\n",
    "\n",
    "her_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the vector for each sampled word\n",
    "\n",
    "vectors = [model[word] for word in her_tokens]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distances among texts in vector space\n",
    "\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multi-Dimensional Scaling (Project vectors into 2-D)\n",
    "\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fussing with matplotlib\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For comparison, here is the same graph using a masculine-pronoun vector\n",
    "\n",
    "his_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n",
    "                                                       negative=['she','her','hers','herself'], topn=50)]\n",
    "vectors = [model[word] for word in his_tokens]\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q. What kinds of semantic relationships exist in the diagram above?\n",
    "##    Are there any words that seem out of place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/fem_vectors.png\", width=\"50%\",style=\"float: left;\" /><img src=\"resources/masc_vectors.png\", width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving/Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save current model for later use\n",
    "\n",
    "model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up models from disk\n",
    "\n",
    "# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n",
    "# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n",
    "\n",
    "ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are similar words to BANK?\n",
    "\n",
    "ecco_model.most_similar('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What if we remove the sense of \"river bank\"?\n",
    "\n",
    "ecco_model.most_similar(positive=['bank'], negative=['river'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n",
    "##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n",
    "##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n",
    "\n",
    "##  Q. How might we compare word2vec models more generally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n",
    "\n",
    "ecco_model.most_similar(positive=['virtue', 'learning'], negative=['riches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\n",
    "\n",
    "model.most_similar(positive=['virtue', 'learning'], negative=['riches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Open Questions\n",
    "At this point, we have seen a number of mathemetical operations that we may use to explore word2vec's word embeddings. These enable us to answer a set of new, interesting questions dealing with semantics, yet there are many other questions that remain unanswered.\n",
    "\n",
    "For example:\n",
    "<ol>\n",
    "<li>How to compare word usages in different texts (within the same model)?</li>\n",
    "<li>How to compare word meanings in different models? compare whole models?</li>\n",
    "<li>What about the space “in between” words?</li>\n",
    "<li>Do we agree with the Distributional Hypothesis that words with the same contexts share their meanings?</li>\n",
    "<ol><li>If not, then what information do we think is encoded in a word’s context?</li></ol>\n",
    "<li>What good, humanistic research questions do analogies shed light on?</li>\n",
    "<ol><li>shades of meaning?</li><li>context similarity?</li></ol>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
